# -*- coding: utf-8 -*-
"""Predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v4QTZF4akH93ks2mnsX6OofcUnORCXdL
"""

import os
import time
import logging
import numpy as np
import tensorflow as tf

def checked_flow_from_dataframe(datagen, df, subset_name, **kwargs):
    print(f"=== Construyendo generador para {subset_name} ===")
    for f in tqdm(df["filename"], desc=f"Verificando {subset_name}", unit="img"):
        if not os.path.exists(f):
            raise FileNotFoundError(f"Imagen no encontrada: {f}")
    generator = datagen.flow_from_dataframe(df, **kwargs)
    return generator

## Limpieza: se eliminaron bloques de entrenamiento/generadores/oversampling e impresi√≥n de dataset.

import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import os
import json
import tensorflow as tf
## Limpieza de imports de entrenamiento/modelado no usados
from flask import Flask, request, jsonify
from pyngrok import ngrok
import base64
import io
from PIL import Image
from collections import Counter, defaultdict

############################
# Servicio Flask por moda  #
############################

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("colab-predict")

# Directorio de modelos (puedes cambiar por env MODEL_DIR)
MODEL_DIR = os.environ.get("MODEL_DIR", "drive/MyDrive/modelo_multiclase").strip()

def load_all_models(model_dir: str):
    models = []
    if not os.path.exists(model_dir):
        logger.warning(f"Modelo dir no existe: {model_dir}")
        return models
    for fname in os.listdir(model_dir):
        if fname.endswith(".keras") or fname.endswith(".h5"):
            fpath = os.path.join(model_dir, fname)
            try:
                m = tf.keras.models.load_model(fpath)
                models.append(m)
                logger.info(f"‚úÖ Modelo cargado: {fpath} | input_shape={m.input_shape}")
            except Exception as e:
                logger.exception(f"‚ùå Error cargando {fpath}: {e}")
    return models

MODELS = load_all_models(MODEL_DIR)
logger.info(f"Modelos cargados: {len(MODELS)}")


def preprocess_efficientnet_300(pil_img: Image.Image) -> np.ndarray:
    img = pil_img.convert("RGB").resize((300, 300))
    arr = image.img_to_array(img)
    arr = np.expand_dims(arr, axis=0)
    arr = tf.keras.applications.efficientnet.preprocess_input(arr)
    return arr

def get_class_names():
    """Obtener nombres de clase de forma consistente"""
    class_names = None
    try:
        ci = train_generator.class_indices  # type: ignore[name-defined]
        class_names = {v: k for k, v in ci.items()}
    except Exception:
        env = os.environ.get("CLASS_NAMES", "").strip()
        if env:
            tmp = [c.strip() for c in env.split(",") if c.strip()]
            if tmp:
                class_names = {i: n for i, n in enumerate(tmp)}
    # Por defecto (si no se defini√≥ nada), usar binario: 0:notumor, 1:tumor
    if class_names is None:
        class_names = {0: "notumor", 1: "tumor"}
    return class_names

def predict_voting(image_bytes: bytes):
    if not MODELS:
        raise RuntimeError("No hay modelos cargados en el servidor")
    t0 = time.time()
    logger.info(f"predict_voting: bytes={len(image_bytes)}")
    pil_img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    x = preprocess_efficientnet_300(pil_img)
    logger.info(f"preprocess listo: shape={x.shape}")

    class_names = get_class_names()
    per_model_class_indices = []
    per_model_selected_probs = []
    class_index_to_probs = defaultdict(list)

    for idx, m in enumerate(MODELS):
        t1 = time.time()
        preds = m.predict(x, verbose=0)
        if isinstance(preds, (list, tuple)):
            preds = preds[0]
        preds = np.array(preds)
        if preds.ndim == 2 and preds.shape[0] == 1:
            vec = preds[0]
        elif preds.ndim == 1:
            vec = preds
        else:
            vec = preds.reshape(-1)
        if vec.shape[0] == 1:
            p1 = float(vec[0])
            vec = np.array([1.0 - p1, p1], dtype=np.float32)
        s = float(np.sum(vec))
        if (np.any(vec < 0) or abs(s - 1.0) > 1e-3) and vec.size > 1:
            ex = np.exp(vec - np.max(vec))
            vec = ex / np.sum(ex)
        cls = int(np.argmax(vec))
        p = float(vec[cls])
        per_model_class_indices.append(cls)
        per_model_selected_probs.append(p)
        class_index_to_probs[cls].append(p)
        logger.info(f"modelo[{idx}] -> cls={cls} prob={p:.4f} tiempo={(time.time()-t1):.3f}s")

    counts = Counter(per_model_class_indices)
    max_votes = max(counts.values())
    candidates = [c for c, v in counts.items() if v == max_votes]
    if len(candidates) == 1:
        final_cls = candidates[0]
    else:
        means = {c: float(np.mean(class_index_to_probs.get(c, [0.0]))) for c in candidates}
        best_mean = max(means.values())
        best_candidates = [c for c, m in means.items() if m == best_mean]
        final_cls = min(best_candidates)

    if class_names and final_cls in class_names:
        predicted_class_name = class_names[final_cls]
    else:
        predicted_class_name = str(final_cls)
    confidence = float(np.mean(class_index_to_probs.get(final_cls, per_model_selected_probs)))
    logger.info(f"final -> cls={final_cls} label={predicted_class_name} conf={confidence:.4f} total_time={(time.time()-t0):.3f}s")
    return predicted_class_name, confidence, counts, per_model_class_indices, per_model_selected_probs





@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "ok", "service": "predict"})

@app.route('/predict', methods=['POST'])
def predict_api():
    try:
        logger.info("/predict recibido")
        data = request.get_json(force=True)
        image_b64 = data.get('image_data')
        if not image_b64:
            logger.warning("/predict falta image_data")
            return jsonify({"status": "error", "error": "Falta image_data base64"}), 400
        image_bytes = base64.b64decode(image_b64)
        logger.info(f"/predict base64_len={len(image_b64)} bytes={len(image_bytes)} modelos={len(MODELS)}")
        label, conf, counts, _, _ = predict_voting(image_bytes)
        resp = {
            "status": "success",
            "prediction": label,
            "mean_score": conf,
            "votes": {str(k): int(v) for k, v in counts.items()}
        }
        logger.info(f"/predict respuesta: {resp}")
        return jsonify(resp)
    except Exception as e:
        logger.exception("/predict error")
        return jsonify({"status": "error", "error": str(e)}), 500

@app.route('/predict-raw', methods=['POST'])
def predict_raw_api():
    try:
        logger.info("/predict-raw recibido")
        data = request.get_json(force=True)
        image_b64 = data.get('image_data')
        if not image_b64:
            logger.warning("/predict-raw falta image_data")
            return jsonify({"status": "error", "error": "Falta image_data base64"}), 400
        image_bytes = base64.b64decode(image_b64)
        logger.info(f"/predict-raw base64_len={len(image_b64)} bytes={len(image_bytes)} modelos={len(MODELS)}")
        label, conf, counts, class_indices, selected_probs = predict_voting(image_bytes)
        resp = {
            "status": "success",
            "prediction": label,
            "mean_score": conf,
            "votes": {str(k): int(v) for k, v in counts.items()},
            "per_model": {
                "class_indices": class_indices,
                "selected_probs": selected_probs,
            }
        }
        logger.info(f"/predict-raw respuesta: {resp}")
        return jsonify(resp)
    except Exception as e:
        logger.exception("/predict-raw error")
        return jsonify({"status": "error", "error": str(e)}), 500



if __name__ == '__main__':
    port = int(os.environ.get('PORT', '8000'))
    # Iniciar Flask en hilo aparte y abrir t√∫nel ngrok si se proporciona token
    public_url = None
    token = os.environ.get('NGROK_AUTH_TOKEN', '').strip()
    if token:
        try:
            ngrok.set_auth_token(token)
            public_url = ngrok.connect(port, "http").public_url
            print(f"üåç URL p√∫blica ngrok: {public_url}")
            print(f"Endpoint predict: {public_url}/predict")
            print(f"Endpoint predict-raw: {public_url}/predict-raw")
        except Exception as e:
            print("No se pudo abrir ngrok:", e)
    app.run(host='0.0.0.0', port=port, debug=False)