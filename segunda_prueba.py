# -*- coding: utf-8 -*-
"""Segunda Prueba.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13JuYpVpUS6ZwTuY0DVZGaGVJciyRpXoQ

# Primero vamos a cargar este primer dataset
"""

from google.colab import files
files.upload() # aqu√≠ subimos el json de la api de kaggle

!pip install -q kaggle

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json # lo guardamos

"""# Dataset Principal ya Fusionado"""

!kaggle datasets download -d gonzajl/tumores-cerebrales-mri-dataset

!unzip -q tumores-cerebrales-mri-dataset.zip -d dataset_final

"""# Vamos a explorar sus carpetas"""

import os
import numpy as np
from tqdm import tqdm

for root, dirs, files in os.walk("dataset_final"):
    print(f"\nüìÅ Carpeta: {root}")
    for d in dirs:
        print(f"  üìÇ {d}")
    for f in files[:3]:
        print(f"  üìÑ {f}")

"""# Una vez tenemos claro la estructura de las carpetas, nos interesa fusionar ambos dataset (teniendo en cuenta que de BRISC solo nos interesa las carpetas asociadas a classification_task)"""

import os, shutil
from sklearn.model_selection import train_test_split

base_dir = "dataset_final/tumores-cerebrales-mri-dataset"
output_dir = "dataset_split_final"

# Crear carpetas base
for subset in ["train", "val", "test"]:
    os.makedirs(os.path.join(output_dir, subset), exist_ok=True)

classes = os.listdir(base_dir)

for cls in classes:
    imgs = os.listdir(os.path.join(base_dir, cls))

    # Primero separar 15% para val+test
    train_files, temp_files = train_test_split(imgs, test_size=0.15, random_state=42)

    # De ese 15%, separar en 10% val y 5% test (proporcional)
    val_files, test_files = train_test_split(temp_files, test_size=1/3, random_state=42)
    # 1/3 ‚âà 5% de total ‚Üí test, 2/3 ‚âà 10% de total ‚Üí val

    # Crear carpetas para cada clase
    for subset in ["train", "val", "test"]:
        os.makedirs(os.path.join(output_dir, subset, cls), exist_ok=True)

    # Copiar im√°genes
    for f in train_files:
        shutil.copy(os.path.join(base_dir, cls, f), os.path.join(output_dir, "train", cls))
    for f in val_files:
        shutil.copy(os.path.join(base_dir, cls, f), os.path.join(output_dir, "val", cls))
    for f in test_files:
        shutil.copy(os.path.join(base_dir, cls, f), os.path.join(output_dir, "test", cls))

dataset_split = "dataset_split_final"  # ruta base

for split in ["train", "val", "test"]:
    print(f"\nüìÇ {split.upper()}")
    for cls in os.listdir(os.path.join(dataset_split, split)):
        count = len(os.listdir(os.path.join(dataset_split, split, cls)))
        print(f"  {cls}: {count} im√°genes")

"""# YA TENEMOS PREPARADO NUESTRO DATASET PREPARADO PARA COMENZAR A TRABAJAR. EN ESTA OCASI√ìN VOY A PROBAR A UTILIZAR EL MULTICLASE TRAS LOS RESULTADOS DECEPCIONANTES DEL BINARIO

En nuestro caso vamos a comenzar utilizando la EfficientNetB3
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, save_img
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import os

"""# Configuramos paths y par√°metros (A PARTIR DE AQU√ç SI)"""

# Global variables
SAVE = False
SEED = 115

# Setting seed for consistent results
tf.keras.utils.set_random_seed(SEED)
tf.random.set_seed(SEED)
np.random.seed(SEED)

# Paths a tu dataset binario
DATASET_DIR = "dataset_split_final"
TRAIN_DIR = os.path.join(DATASET_DIR, "train")
VAL_DIR = os.path.join(DATASET_DIR, "val")
TEST_DIR = os.path.join(DATASET_DIR, "test")

# Par√°metros del modelo
IMG_SIZE = 300  # EfficientNetB3 usa 300x300
BATCH_SIZE = 64
EPOCHS = 10

"""Preprocesamiento y generadores"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,            # rotaci√≥n leve
    brightness_range=(0.85, 1.15),# variaci√≥n de brillo moderada
    width_shift_range=0.02,
    height_shift_range=0.02,
    shear_range=0.0,              # sin deformaci√≥n de corte
    zoom_range=0.05,              # zoom leve
    horizontal_flip=True,         # solo si anat√≥micamente es v√°lido
    vertical_flip=False,          # mejor no en MRI
    fill_mode="nearest"
)


val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    seed=SEED
)

val_generator = val_datagen.flow_from_directory(
    VAL_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False,
    seed=SEED
)

"""Para saber a que corresponde cada clase"""

# Accessing class indices for training data generator
class_indices_train = train_generator.class_indices
class_indices_train_list = list(train_generator.class_indices.keys())


# Displaying categorical types
print("Categorical types for the training data:")
print(class_indices_train)

"""Cargamos EfficientNetB3 y modificamos la cabeza"""

# base model sin la parte final (sin top)
base_model = EfficientNetB3(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)
base_model.trainable = False  # Transfer learning (entrenamos solo la parte nueva, voy a probar true)

# nuevas capitas
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.4)(x)
x = Dense(4, activation="softmax")(x)  # 4 clases

model = Model(inputs=base_model.input, outputs=x)

"""Compilamos el modelo"""

model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

###################### UTILIZAR SOLO sI SIGUE SIN MEJORAR EN EL FINE TUNING USAREMOS ESTO EN LUGAR DE ADAM
#########################################################################################################
#from tensorflow.keras.optimizers import SGD                         #####################################
#                                                                    #####################################
#model.compile(                                                      #####################################
#    optimizer=SGD(learning_rate=0.01, momentum=0.9),                #####################################
#    loss='categorical_crossentropy',                                #####################################
#    metrics=['accuracy']                                            #####################################
#)                                                                   #####################################
#########################################################################################################

"""Entrenamos el modelo (Dado el tama√±o de nuestra base de datos cada √©poca va a tardar bastante)"""

history = model.fit(
    train_generator,
    epochs=EPOCHS,     # En este caso con el nuevo dataset tardar√° m√°s a√∫n
    validation_data=val_generator # otro modelo menos potente podr√≠a acelerar las pruebas tambien
)

"""# Fine-tuning de EfficientNetB3. VAMOS A TRABAJAR EN DIFERENTES FASES CONGELANDO CAPAS EN FUNCI√ìN DE NUESTRAS NECESIDADES Y ESTRUCTURA DE LA RED"""

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam

# Callbacks comunes para todas las fases
reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=7,
    restore_best_weights=True
)

"""Preparados que se viene fumada a continuaci√≥n"""

# Funci√≥n para ejecutar cada fase
def train_phase(freeze_until, lr, epochs, phase_name):
    print(f"\nüîπ {phase_name} ‚Üí Congelando capas hasta {freeze_until}")

    base_model.trainable = True
    for layer in base_model.layers[:freeze_until]:
        layer.trainable = False

    model.compile(
        optimizer=Adam(learning_rate=lr),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=epochs,
        callbacks=[early_stop, reduce_lr]
    )

    model.save(f"{phase_name}.h5")  # Guarda el modelo por fase
    return history

"""Lo que vamos a hacer a continuaci√≥n es ir descongelando capas seg√∫n nuestra necesidad para prevenir el overfitting y seguir perfeccionando el modelo

FASE 1 ‚Üí √∫ltimo bloque (block7a): √≠ndice 354 (block7a_expand_conv)
"""

# ===== FASE 1: solo √∫ltimo bloque =====
history1 = train_phase(
    freeze_until=354,
    lr=1e-5,
    epochs=15,
    phase_name="fase1"
)

# Guardamos modelo aunque no sea √≥ptimo a√∫n
model.save("modelo_multiclase.h5")

"""FASE 2 ‚Üí pen√∫ltimo bloque (block6a): √≠ndice 265 (block6a_expand_conv)"""

# ===== FASE 2: √∫ltimos dos bloques =====
history2 = train_phase(
    freeze_until=265,  # Ajusta al √≠ndice del bloque 6
    lr=1e-5,
    epochs=15,
    phase_name="fase2"
)

# guardamos este modelo por si acaso, el seiguiente entreno puede ser m√°s agresivo
model.save("modelo_multiclase.h5")

"""FASE 3 ‚Üí desbloquear todo menos primeros 50‚Äì100 layers: si quieres dejar primeras 100 fijas, √≠ndice 100 (block3b_project_bn)"""

# ===== FASE 3: todo menos primeras 100 capas =====
history3 = train_phase(
    freeze_until=100,
    lr=1e-6,  # m√°s bajo para no destrozar pesos
    epochs=15,
    phase_name="fase3"
)

# guardamos este modelo
model.save("modelo_multiclase.h5")

# y el historial por si queremos graficar etc
import pickle
with open("historial.pkl", "wb") as f:
    pickle.dump(history.history, f)

"""Entreno final"""

# Desbloquear capas superiores del modelo base para fine-tuning
base_model.trainable = True

# Congela las primeras 400 capas (esto depende del modelo base; ajusta si necesario)
for layer in base_model.layers[:200]:  # esto ha sido clave para reducir el val loss y aumentar val acc
    layer.trainable = False

# Recompila el modelo con tasa de aprendizaje m√°s baja y p√©rdida correcta
model.compile(
    optimizer=Adam(learning_rate=1e-5),  # fine-tuning m√°s lento
    loss='categorical_crossentropy',    # CORRECTO para multiclase
    metrics=['accuracy']
)

from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',           # puedes usar 'val_accuracy' si te interesa m√°s
    patience=7,                   # espera 5 √©pocas sin mejora
    restore_best_weights=True     # vuelve a los mejores pesos al final
)

# y otra vez a llorar (fine-tune) lo paro porque no mejora una mierda hay que equilibrar las clases
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=70,
    callbacks=[early_stop, reduce_lr]
)

"""# M√©tricas de nuestro modelo en su mejor √©poca"""

# Mejoras en validaci√≥n
best_val_acc = max(history.history['val_accuracy'])
best_val_loss = min(history.history['val_loss'])

# Mejoras en entrenamiento
best_acc = max(history.history['accuracy'])
best_loss = min(history.history['loss'])

print(f"Mejor accuracy (train): {best_acc:.4f}")
print(f"Mejor loss (train): {best_loss:.4f}")
print(f"Mejor val_accuracy: {best_val_acc:.4f}")
print(f"Mejor val_loss: {best_val_loss:.4f}")

"""# PARA CARGAR EL MODELO"""

from tensorflow.keras.models import load_model

# cargar el modelo previamente entrenado
model = load_model("modelo_multiclase.h5")

"""# PARA VISUALIZARLO"""

!pip install visualkeras

from tensorflow.keras.utils import plot_model
import visualkeras

# Para verlo a modo esquem√°tico
plot_model(model,
           to_file='model.png',
           show_shapes=True,
           show_layer_names=True)

# Para ver la representaci√≥n gr√°fica
visualkeras.layered_view(model, legend=True)

"""No se ve una mierda pero si clickais en la imagen se agranda bastante, esta guay para explicar esta red

# Para volver a entrenar sin necesidad de repetir todo el proceso anterior
"""

# Esto nos puede ayudar a controlar el val loss
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',       # M√©trica a vigilar
    factor=0.5,               # Reduce LR a la mitad
    patience=3,               # Espera 3 epochs sin mejora
    min_lr=1e-7,              # No baja m√°s de esto
    verbose=1                 # Muestra por pantalla cu√°ndo act√∫a
)

# recompilar (√∫til para cambiar learning rate, optimizer, etc)
model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# volver a entrenar
history_2 = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[early_stop, reduce_lr]
)

"""# Gr√°fica de Accuracy y Loss


"""

import matplotlib.pyplot as plt

def plot_training(history, title="Entrenamiento"):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs_range = range(len(acc))

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Train Accuracy')
    plt.plot(epochs_range, val_acc, label='Val Accuracy')
    plt.legend()
    plt.title(f'{title} - Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Train Loss')
    plt.plot(epochs_range, val_loss, label='Val Loss')
    plt.legend()
    plt.title(f'{title} - Loss')

    plt.show()

plot_training(history, "EfficientNetB3")

"""# Matriz de confusi√≥n"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Predicciones del modelo
y_pred_probs = model.predict(val_generator)
y_pred = np.argmax(y_pred_probs, axis=1)  # Clase con mayor probabilidad

y_true = val_generator.classes
class_labels = list(val_generator.class_indices.keys())

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)

plt.figure(figsize=(10, 8))
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title("Matriz de Confusi√≥n")
plt.show()

"""Aqu√≠ vemos algo muy importante, glioma es la clase peor predicha y justo es la clase que menos elementos tiene. Si conseguimos equilibrar esta clase junto al resto cabr√≠a esperar incluso un mejor ajuste. A√∫n as√≠ en nuestro caso, realmente nos interesa distinguir entre tumor o no, y si asumimos cualquier etiqueta diferente a no tumor como tumor nuestro accuracy es practicamente perfecto.

# Para predecir EL TEST
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Generador para test (sin aumentos)
test_datagen = ImageDataGenerator(rescale=1./255)

# Cargamos test
test_generator = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False  # importante para que las predicciones correspondan a los ficheros en orden
)

# Evaluar
loss, acc = model.evaluate(test_generator)
print(f"üìä Test accuracy: {acc:.4f}")
print(f"üìâ Test loss: {loss:.4f}")

"""# FUNCI√ìN PARA PREDECIR IM√ÅGENES INDIVIDUALES

Esta funci√≥n te permite subir una imagen y obtener la predicci√≥n de si es un tumor o no.
"""

def predecir_tumor_imagen(ruta_imagen=None):
    """
    Funci√≥n para predecir si una imagen contiene un tumor cerebral o no.
    
    Args:
        ruta_imagen (str): Ruta de la imagen a analizar. Si es None, permite subir una imagen.
    
    Returns:
        dict: Diccionario con la predicci√≥n y probabilidades
    """
    import matplotlib.pyplot as plt
    from tensorflow.keras.preprocessing.image import load_img, img_to_array
    from tensorflow.keras.models import load_model
    import numpy as np
    
    # Cargar el modelo entrenado
    try:
        modelo = load_model("modelo_multiclase.h5")
        print("‚úÖ Modelo cargado correctamente")
    except:
        print("‚ùå Error: No se encontr√≥ el modelo 'modelo_multiclase.h5'")
        print("   Aseg√∫rate de haber entrenado el modelo primero")
        return None
    
    # Si no se proporciona ruta, permitir subir imagen
    if ruta_imagen is None:
        print("üìÅ Sube una imagen para analizar:")
        uploaded = files.upload()
        if not uploaded:
            print("‚ùå No se subi√≥ ninguna imagen")
            return None
        ruta_imagen = list(uploaded.keys())[0]
        print(f"üìÑ Imagen subida: {ruta_imagen}")
    
    # Cargar y preprocesar la imagen
    try:
        img = load_img(ruta_imagen, target_size=(IMG_SIZE, IMG_SIZE))
        img_array = img_to_array(img)
        img_array = img_array / 255.0  # Normalizar
        img_array = np.expand_dims(img_array, axis=0)  # A√±adir dimensi√≥n de batch
        
        print("‚úÖ Imagen cargada y preprocesada correctamente")
    except Exception as e:
        print(f"‚ùå Error al cargar la imagen: {e}")
        return None
    
    # Hacer predicci√≥n
    predicciones = modelo.predict(img_array)
    clase_predicha = np.argmax(predicciones[0])
    confianza = predicciones[0][clase_predicha]
    
    # Obtener nombres de las clases (asumiendo que est√°n en el mismo orden que el entrenamiento)
    clases = ['glioma', 'meningioma', 'no_tumor', 'pituitary']
    
    # Determinar si es tumor o no
    es_tumor = clase_predicha != 2  # √≠ndice 2 corresponde a 'no_tumor'
    
    # Mostrar resultados
    print("\n" + "="*50)
    print("üîç RESULTADO DEL AN√ÅLISIS")
    print("="*50)
    
    if es_tumor:
        print("üö® SE HA DETECTADO UN TUMOR CEREBRAL")
        print(f"üìã Tipo de tumor: {clases[clase_predicha].upper()}")
    else:
        print("‚úÖ NO SE HA DETECTADO NING√öN TUMOR")
        print(f"üìã Diagn√≥stico: {clases[clase_predicha].upper()}")
    
    print(f"üéØ Confianza: {confianza:.2%}")
    
    # Mostrar todas las probabilidades
    print("\nüìä PROBABILIDADES POR CLASE:")
    for i, (clase, prob) in enumerate(zip(clases, predicciones[0])):
        emoji = "üö®" if i != 2 else "‚úÖ"  # tumor vs no tumor
        print(f"   {emoji} {clase}: {prob:.2%}")
    
    # Visualizar la imagen
    plt.figure(figsize=(10, 5))
    
    plt.subplot(1, 2, 1)
    plt.imshow(img)
    plt.title(f"Imagen Analizada\n{clases[clase_predicha].upper()}")
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    barras = plt.bar(clases, predicciones[0])
    plt.title("Probabilidades por Clase")
    plt.ylabel("Probabilidad")
    plt.xticks(rotation=45)
    
    # Colorear la barra m√°s alta
    barras[clase_predicha].set_color('red' if es_tumor else 'green')
    
    plt.tight_layout()
    plt.show()
    
    # Retornar resultados
    return {
        'es_tumor': es_tumor,
        'clase_predicha': clases[clase_predicha],
        'confianza': confianza,
        'probabilidades': dict(zip(clases, predicciones[0].tolist()))
    }

# Funci√≥n simplificada para uso r√°pido
def analizar_imagen():
    """
    Funci√≥n simplificada para analizar una imagen subida.
    """
    print("üß† AN√ÅLISIS DE TUMORES CEREBRALES")
    print("="*40)
    resultado = predecir_tumor_imagen()
    
    if resultado:
        if resultado['es_tumor']:
            print(f"\n‚ö†Ô∏è  RECOMENDACI√ìN: Consultar con un m√©dico especialista")
        else:
            print(f"\n‚úÖ RECOMENDACI√ìN: Continuar con revisiones rutinarias")
    
    return resultado

# Ejemplo de uso:
# resultado = predecir_tumor_imagen("ruta/a/tu/imagen.jpg")
# O simplemente: analizar_imagen()  # para subir una imagen interactivamente